{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging two dataframes\n",
    "\n",
    "Here we do a simple merge between two dataframes concerning tube stations. The two files look like:\n",
    "\n",
    "`stations.csv`\n",
    "```\n",
    "Latitude,Longitude,Station Name\n",
    "51.5028,-0.2801,\"Acton Town\"\n",
    "51.5143,-0.0755,\"Aldgate\"\n",
    "51.5154,-0.0726,\"Aldgate East\"\n",
    "51.5107,-0.013,\"All Saints\"\n",
    "51.5407,-0.2997,\"Alperton\"\n",
    "51.5322,-0.1058,\"Angel\"\n",
    "```\n",
    "\n",
    "`Station Stats.csv`\n",
    "\n",
    "```\n",
    "Year,Station,Entry-Weekday,Entry-Sat,Entry-Sun,Exit-Weekday,Exit-Sat,Exit-Sun,Entry & Exit-Annual\n",
    "2007,Acton Town                         ,9205,6722,4427,8899,6320,4304,359.73\n",
    "2007,Aldgate                            ,9887,2191,1484,10397,2587,1772,302.88\n",
    "2007,Aldgate East                       ,12820,7040,5505,12271,6220,5000,458.48\n",
    "2007,Alperton                           ,4611,3354,2433,4719,3450,2503,188.00\n",
    "```\n",
    "\n",
    "In this notebook we shall\n",
    "1. Import the two `.csv` files\n",
    "2. Clean up the extra spaces in `Station Stats.csv`\n",
    "3. Merge the dataframes\n",
    "4. Upload the merged dataframe to Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /anaconda3/lib/python3.6/site-packages (0.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /anaconda3/lib/python3.6/site-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in /anaconda3/lib/python3.6/site-packages (from pandas) (2018.4)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /anaconda3/lib/python3.6/site-packages (from pandas) (1.14.3)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: count-api in /anaconda3/lib/python3.6/site-packages (3.0.4)\n",
      "Requirement already satisfied: future in /anaconda3/lib/python3.6/site-packages (from count-api) (0.16.0)\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.6/site-packages (from count-api) (2.18.4)\n",
      "Requirement already satisfied: protobuf==3.5.1 in /anaconda3/lib/python3.6/site-packages (from count-api) (3.5.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests->count-api) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests->count-api) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests->count-api) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests->count-api) (2018.4.16)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from protobuf==3.5.1->count-api) (39.1.0)\n",
      "Requirement already satisfied: six>=1.9 in /anaconda3/lib/python3.6/site-packages (from protobuf==3.5.1->count-api) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install count-api\n",
    "import pandas as pd\n",
    "from count_api import CountAPI\n",
    "\n",
    "# Set this to the local path of the GitHub repository\n",
    "data_dir = '../data/'\n",
    "token = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6IkpjUnVRZjFSVWhqbHdSZVlTRk1IZkBjb3VudC5jbyIsImp3dGlkIjoiZkVkQnFzWkNZNkNpY0JaTWh-VEhrIiwiaWF0IjoxNTQzNDExODA1LCJleHAiOjE1NzQ5NDc4MDUsImF1ZCI6Imh0dHBzOi8vcGxheS5jb3VudC5jbyJ9.fkOURFrH3KP6dbSJ5Vw0UPzyu3FbIEtY0rLe_BmSILE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/path/to/open-data-scripts/december-2018-hackathon/Transportation/stations.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-abf5ac1ac45b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstation_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Transportation/stations.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstation_entry_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Transportation/Station Stats.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/path/to/open-data-scripts/december-2018-hackathon/Transportation/stations.csv' does not exist"
     ]
    }
   ],
   "source": [
    "station_locations = pd.read_csv(data_dir + 'Transportation/stations.csv')\n",
    "station_entry_stats = pd.read_csv(data_dir + 'Transportation/Station Stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Latitude  Longitude  Station Name\n",
      "0   51.5028    -0.2801    Acton Town\n",
      "1   51.5143    -0.0755       Aldgate\n",
      "2   51.5154    -0.0726  Aldgate East\n",
      "3   51.5107    -0.0130    All Saints\n",
      "4   51.5407    -0.2997      Alperton\n"
     ]
    }
   ],
   "source": [
    "print(station_locations.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year                              Station  Entry-Weekday  Entry-Sat  \\\n",
      "0  2007  Acton Town                                    9205       6722   \n",
      "1  2007  Aldgate                                       9887       2191   \n",
      "2  2007  Aldgate East                                 12820       7040   \n",
      "3  2007  Alperton                                      4611       3354   \n",
      "4  2007  Amersham                                      4182       1709   \n",
      "\n",
      "   Entry-Sun  Exit-Weekday  Exit-Sat  Exit-Sun  Entry & Exit-Annual  \n",
      "0       4427        8899.0      6320      4304               359.73  \n",
      "1       1484       10397.0      2587      1772               302.88  \n",
      "2       5505       12271.0      6220      5000               458.48  \n",
      "3       2433        4719.0      3450      2503               188.00  \n",
      "4       1004        3938.0      1585       957               134.14  \n"
     ]
    }
   ],
   "source": [
    "print(station_entry_stats.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the data\n",
    "\n",
    "You'll note above that the `Station` column in `station_entry_stats` has a load of whitespace on the right-hand-side. We'll want to compare station names in the next step, and this will mess things up. Here we'll use the inbuilt python function `strip()` to remove this unnecessary whitespace.\n",
    "\n",
    "We use the `apply(func)` method on a dataframe column, where `func` is a function to apply to each element in the column. For the case where `func` is very simple (a single statement), we can use an inline [lambda](https://docs.python.org/3.5/tutorial/controlflow.html#lambda-expressions) function to save precious keystrokes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year       Station  Entry-Weekday  Entry-Sat  Entry-Sun  Exit-Weekday  \\\n",
      "0  2007    Acton Town           9205       6722       4427        8899.0   \n",
      "1  2007       Aldgate           9887       2191       1484       10397.0   \n",
      "2  2007  Aldgate East          12820       7040       5505       12271.0   \n",
      "3  2007      Alperton           4611       3354       2433        4719.0   \n",
      "4  2007      Amersham           4182       1709       1004        3938.0   \n",
      "\n",
      "   Exit-Sat  Exit-Sun  Entry & Exit-Annual  \n",
      "0      6320      4304               359.73  \n",
      "1      2587      1772               302.88  \n",
      "2      6220      5000               458.48  \n",
      "3      3450      2503               188.00  \n",
      "4      1585       957               134.14  \n"
     ]
    }
   ],
   "source": [
    "station_entry_stats['Station'] = station_entry_stats['Station'].apply(lambda x: x.strip())\n",
    "print(station_entry_stats.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merging the dataframes\n",
    "\n",
    "Now that the station names in each dataframe are the same, we are ready to perform the merge. In the following line, we select the two dataframes (`left` and `right`), the columns to merge on (`left_on` and `right_on`), and then delete a column with duplicate information (`Station Name`).\n",
    "\n",
    "Finally, we save the dataframe to a CSV file, `merged.csv`. (Setting `index=False` just means we don't save the row numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Latitude  Longitude  Year     Station  Entry-Weekday  Entry-Sat  Entry-Sun  \\\n",
      "0   51.5028    -0.2801  2007  Acton Town           9205       6722       4427   \n",
      "1   51.5028    -0.2801  2008  Acton Town           9285       6574       4358   \n",
      "2   51.5028    -0.2801  2009  Acton Town           8601       5816       4231   \n",
      "3   51.5028    -0.2801  2010  Acton Town           8669       5912       4184   \n",
      "4   51.5028    -0.2801  2011  Acton Town           8702       6326       4216   \n",
      "\n",
      "   Exit-Weekday  Exit-Sat  Exit-Sun  Entry & Exit-Annual  \n",
      "0        8899.0      6320      4304               359.73  \n",
      "1        9028.0      6295      4361               361.05  \n",
      "2        8595.0      5803      4324               337.69  \n",
      "3        8403.0      5877      4224               336.58  \n",
      "4        8392.0      5976      4223               340.57  \n"
     ]
    }
   ],
   "source": [
    "station_merged = pd.merge(left=station_locations, right=station_entry_stats, left_on='Station Name', right_on='Station').drop('Station Name', axis=1)\n",
    "print(station_merged.sort_values(by='Station').head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_merged.to_csv('merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Uploading to Count\n",
    "\n",
    "The fun bit! Import the Count API module, and initialise it with your access token, then upload the file saved in step 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountAPI()\n",
    "count.set_api_token(token)\n",
    "table = count.upload(path='./merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, with the table uploaded, create an interactive plot of stations by location, coloured by the number of passengers entering each one during weekdays. Click on the Count logo to explore the data further, or modify the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1000\"\n",
       "            src=\"https://count.co/embed/HZT1RQqYVD1?v=2d1UfAQZokK&view=visual&x_type=linear&y_type=linear&color_type=log&x=0&y=1&color=2\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<count_api.visual.visual.IFrame at 0x115013630>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual = table.upload_visual(x=table['Longitude'], y=table['Latitude'], color=table['Entry-Weekday'], chart_options={'color_type': 'log'})\n",
    "visual.embed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
